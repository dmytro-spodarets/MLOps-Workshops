import os
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage, AIMessage
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
import murnitur
from murnitur import Guard, GuardConfig, log
from murnitur.guard import RuleSet
from qdrant_client import QdrantClient

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MURNITUR_API_KEY = os.getenv("MURNITUR_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL",
                       "https://3fb2a758-19fe-4158-b88b-41f9259cdcca.europe-west3-0.gcp.cloud.qdrant.io:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Murnitur
murnitur.set_api_key(MURNITUR_API_KEY)
murnitur.init(project_name="MLOps", enabled_instruments=["openai", "langchain", "qdrant"])

config = GuardConfig(api_key=OPENAI_API_KEY, provider="openai", group="MLOps-workshop")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Qdrant
qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
vectorstore = Qdrant(
    client=qdrant_client,
    collection_name="devops_docs",
    embeddings=embeddings,
)

# –õ–æ–≥—É–≤–∞–Ω–Ω—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó Qdrant
log("Qdrant_Initialization", {
    "collection_name": "devops_docs",
    "embeddings_model": "OpenAI",
    "qdrant_url": QDRANT_URL
})

# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è rulesets –¥–ª—è –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
input_rulesets: list[RuleSet] = [
    {
        "rules": [
            {
                "metric": "input_pii",
                "operator": "contains",
                "value": ["financial_info", "email", "ssn"],
            }
        ],
        "action": {
            "type": "OVERRIDE",
            "fallback": "I cannot process requests containing personal identifiable information ü§ê",
        },
    },
    {
        "rules": [
            {
                "metric": "prompt_injection",
                "operator": "contains",
                "value": [
                    "simple_instruction",
                    "instruction_override",
                    "impersonation",
                    "personal_information_request"
                ],
            }
        ],
        "action": {
            "type": "OVERRIDE",
            "fallback": "Sorry, I can't help with that request.",
        },
    },
]

# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è rulesets –¥–ª—è –≤–∏—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
output_rulesets: list[RuleSet] = [
    {
        "rules": [
            {
                "metric": "pii",
                "operator": "contains",
                "value": ["financial_info", "email", "ssn"],
            }
        ],
        "action": {
            "type": "OVERRIDE",
            "fallback": "I cannot provide personal identifiable information ü§ê",
        },
    },
]


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∑–∞–ø–∏—Ç—É —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ –º–æ–¥–µ–ª—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º RAG
def process_query(query: str, model: ChatOpenAI):
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    input_check = Guard.shield({"input": query}, input_rulesets, config)
    if input_check.triggered:
        log("Input_Violation", {"query": query, "violation": input_check.text})
        return input_check.text, True, None

    # –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –¥–æ Qdrant
    log("Qdrant_Query", {"query": query})

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–Ω—Ü—é–∂–∫–∞ RetrievalQA
    qa_chain = RetrievalQA.from_chain_type(
        model,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )

    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ –º–æ–¥–µ–ª—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º RAG
    try:
        result = qa_chain({"query": query})
        response = result['result']
        source_documents = result['source_documents']

        # –õ–æ–≥—É–≤–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ–≥–æ –ø–æ—à—É–∫—É –≤ Qdrant
        log("Qdrant_Search_Success", {
            "query": query,
            "num_source_documents": len(source_documents)
        })
    except Exception as e:
        log("RAG_Error", {"query": query, "error": str(e)})
        return f"An error occurred: {str(e)}", True, None

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∏—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    output_check = Guard.shield({"output": response}, output_rulesets, config)
    if output_check.triggered:
        log("Output_Violation", {"query": query, "response": response, "violation": output_check.text})
        return output_check.text, True, None

    # –õ–æ–≥—É–≤–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ—ó –≤–∑–∞—î–º–æ–¥—ñ—ó
    log("Successful_Interaction", {"query": query, "response": response})

    return response, False, source_documents


# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Streamlit
st.set_page_config(page_title="DevOpsLLM Chat –∑ RAG", page_icon="üí¨")
st.title("DevOpsLLM Chat –∑ RAG")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
model = ChatOpenAI(
    model_name="DevOpsLLM",
    openai_api_base="https://t5-or-phi-model.apps.spodarets.com/v1",
    openai_api_key="not-needed"
)

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó —á–∞—Ç—É
if "messages" not in st.session_state:
    st.session_state.messages = []

# –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó —á–∞—Ç—É
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        st.chat_message("user").write(message.content)
    elif isinstance(message, AIMessage):
        st.chat_message("assistant").write(message.content)

# –ü–æ–ª–µ –≤–≤–µ–¥–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
user_input = st.chat_input("–í–≤–µ–¥—ñ—Ç—å –≤–∞—à–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:")

if user_input:
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó
    st.session_state.messages.append(HumanMessage(content=user_input))
    st.chat_message("user").write(user_input)

    # –û–±—Ä–æ–±–∫–∞ –∑–∞–ø–∏—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –º–æ–¥–µ–ª—ñ, Murnitur —Ç–∞ RAG
    with st.spinner("DevOpsLLM –¥—É–º–∞—î..."):
        response, is_warning, source_documents = process_query(user_input, model)

    if is_warning:
        st.warning(response)
    else:
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –º–æ–¥–µ–ª—ñ –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó
        st.session_state.messages.append(AIMessage(content=response))
        st.chat_message("assistant").write(response)

        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–∂–µ—Ä–µ–ª
        if source_documents:
            st.write("–î–∂–µ—Ä–µ–ª–∞:")
            for doc in source_documents:
                st.write(f"- {doc.metadata.get('source', '–ù–µ–≤—ñ–¥–æ–º–µ –¥–∂–µ—Ä–µ–ª–æ')}")
